{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## _Computer Vision_\n",
    "### Lab 7, _Segmentation and Background Subtraction_.\n",
    "#### >> Topics  :\n",
    "* ##### Image Segmentation\n",
    "  * ###### Adaptive Thresholding\n",
    "  * ###### Otsu Thresholding\n",
    "  * ###### K-Means Clustering\n",
    "* ##### Background Subtraction\n",
    "* ##### Motion Detection\n",
    "\n",
    "##### >> Notes  :\n",
    "> ###### To close image windows smoothly please **press Esc** on your keyboard, **don't close** it directly by clicking on 'X' to avoid kernel interruption.\n",
    "> ###### To run this lab using google colab follow below instructions:\n",
    "> 1. Import `cv2_imshow` method using `from google.colab.patches import cv2_imshow`.\n",
    "> 2. Replace all `cv.imshow()` with `cv2_imshow()` with one parameter **name of that image**.\n",
    "> 3. Specify your path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import cv2 as cv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Section 0, _Helper Functions_:\n",
    "> ##### This cell contains some helper function such as error_handler, please run it without any modification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# please don't modify this code.\n",
    "def show_text_window(titles):\n",
    "    black = np.zeros((len(titles) * 150, 400))\n",
    "    for idx, t in enumerate(titles):\n",
    "        place = idx + 1\n",
    "        cv.putText(black, t, (10, place * 100),\n",
    "                   cv.FONT_HERSHEY_SIMPLEX, 0.75, (200, 0, 200), 1, 2)\n",
    "    cv.imshow(\"Values\", black)\n",
    "\n",
    "\n",
    "def get_updated_value(key, value, **kwargs):\n",
    "    if key == ord('+'):\n",
    "        return value, 0, 0\n",
    "    elif key == ord('-'):\n",
    "        return -value, 0, 0\n",
    "    elif key == ord('*'):\n",
    "        return 0, kwargs.get('value2', value), 0\n",
    "    elif key == ord('/'):\n",
    "        return 0, -kwargs.get('value2', value), 0\n",
    "    elif key == ord('8'):\n",
    "        return 0, 0, kwargs.get('value3', value)\n",
    "    elif key == ord('2'):\n",
    "        return 0, 0, -kwargs.get('value3', value)\n",
    "    elif key == 27:\n",
    "        raise Exception('Terminated by user!')\n",
    "    else:\n",
    "        return 0, 0, 0\n",
    "\n",
    "\n",
    "def error_handler(func):\n",
    "    def wrapper(*args, **kwargs):\n",
    "        try:\n",
    "            return func(*args, **kwargs)\n",
    "        except Exception as ex:\n",
    "            cv.destroyAllWindows()\n",
    "            print(f'{ex}')\n",
    "\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Section 1, _Image Segmentation_:\n",
    "* ##### Is the process if dividing an image into meaningful and semantically homogeneous regions or segments.\n",
    "* #####  The goal is to simplify the representation of an image, making it easier to analyze and understand.\n",
    "* ##### Each segment or region typically corresponds to a specific object or a part of an object within the image.\n",
    "* ##### [Click to check the documentation](https://viso.ai/deep-learning/image-segmentation-using-deep-learning/)\n",
    "* ![image info](https://viso.ai/wp-content/uploads/2022/06/semantic-image-segmentation-of-aerial-images-using-pytorch.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.1 Adaptive Thresholding\n",
    "* ##### Is a technique where the threshold value is not fixed for the entire image but varies based on the local characteristics of the image.\n",
    "* ##### This method is useful when lighting changes across different parts of the image.\n",
    "* ##### [Click to check the documentation](https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html).\n",
    "* ![image info](https://docs.opencv.org/3.4/ada_threshold.jpg)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated by user!\n"
     ]
    }
   ],
   "source": [
    "@error_handler\n",
    "def adaptive_thresholding(img_path):\n",
    "\n",
    "    img = cv.imread(img_path, 0)\n",
    "    img = cv.medianBlur(img, 5)\n",
    "    ret, th1 = cv.threshold(img, 127, 255, cv.THRESH_BINARY)\n",
    "    c = 10\n",
    "    block_size = 11\n",
    "    while True:\n",
    "        th2 = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, block_size, c)\n",
    "        th3 = cv.adaptiveThreshold(img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, block_size, c)\n",
    "        titles = ['Original Image', 'Global Thresholding (v = 127)', 'AdaptiveMeanThresholding',\n",
    "                  'Adaptive Gaussian Thresholding']\n",
    "        images = [img, th1, th2, th3]\n",
    "        for image in zip(titles, images):\n",
    "            img_to_show = cv.resize(image[1], (400, 400))\n",
    "            cv.imshow(image[0],img_to_show)\n",
    "            cv.resizeWindow(image[0], 400, 400)\n",
    "            cv.setWindowProperty(image[0], cv.WND_PROP_TOPMOST, 1)\n",
    "\n",
    "\n",
    "        k = cv.waitKey(0)\n",
    "        uv1, uv2, _ = get_updated_value(k, value=2, value2=2)\n",
    "        block_size += uv1\n",
    "        c += uv2\n",
    "        block_size = max(block_size, 11)\n",
    "\n",
    "path = \"../Images/sudoku.jpg\"\n",
    "adaptive_thresholding(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.2 Otso Thresholding\n",
    "* ##### It is designed to find an optimal threshold to separate an image into two classes, typically foreground and background.\n",
    "* ##### The goal is to minimize the intra-class variance of pixel intensities, effectively maximizing the inter-class variance between the two classes.\n",
    "* ##### This method is useful when there is a bimodal distribution of pixel intensities in the image, meaning there are clear separations between the foreground and background intensities.\n",
    "* ##### [Click to check the documentation](https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html)\n",
    "* * ![image info](https://scipy-lectures.org/_images/sphx_glr_plot_threshold_001.png)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "def otsu_thresholding(path):\n",
    "    img = cv.imread(path, 0)\n",
    "    # global thresholding\n",
    "    ret1, th1 = cv.threshold(img, 127, 255, cv.THRESH_BINARY)\n",
    "    # Otsu's thresholding\n",
    "    ret2, th2 = cv.threshold(img, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "    # Otsu's thresholding after Gaussian filtering\n",
    "    blur = cv.GaussianBlur(img, (5, 5), 0)\n",
    "    ret3, th3 = cv.threshold(blur, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "\n",
    "    images = [img, th1, th2, th3]\n",
    "    titles = ['Original Image', 'Binary Threshold', 'Basic OTSU Threshold', 'Gaussian OTSU Threshold']\n",
    "    for image in zip(titles, images):\n",
    "        cv.imshow(image[0], image[1])\n",
    "        cv.setWindowProperty(image[0], cv.WND_PROP_TOPMOST, 1)\n",
    "    cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "path = \"../Images/coins_1.jpg\"\n",
    "otsu_thresholding(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 1.3 K-Means Clustering.\n",
    "* ##### The basic idea behind K-means clustering is to partition an image into k clusters based on pixel similarities.\n",
    "* ##### Each cluster represents a segment in the image, and pixels within the same cluster share similar characteristics.\n",
    "* ##### [Click to check the documentation](https://docs.opencv.org/3.4/d7/d4d/tutorial_py_thresholding.html)\n",
    "* * ![image info](https://miro.medium.com/max/699/1*XbIyaCstQ2-k0dSmdDYhog.png)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def kmeans_clustering(image_path):\n",
    "    img = cv.imread(image_path)\n",
    "    z = img.reshape((-1, 3))\n",
    "    # convert to np.float32\n",
    "    # define criteria, number of clusters(K)and apply kmeans()\n",
    "    z = np.float32(z)\n",
    "\n",
    "    criteria = (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)\n",
    "    clusters = [1, 2, 4, 8, 10, 12, 20]\n",
    "    for i, k in enumerate(clusters):\n",
    "        ret, label, center = cv.kmeans(z, k, None, criteria, 10, cv.KMEANS_RANDOM_CENTERS)\n",
    "        # Now convert back into uint8, and make original image\n",
    "        center = np.uint8(center)\n",
    "        res = center[label.flatten()]\n",
    "        res2 = res.reshape(img.shape)\n",
    "        cv.imshow(f'{k}', res2)\n",
    "        cv.waitKey(0)\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "path = '../Images/home.jpg'\n",
    "kmeans_clustering(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 2, _Background Subtraction_:\n",
    "* ##### It is used to identify moving objects within a video sequence by isolating them from the background\n",
    "* ##### The primary goal of background subtraction is to segment the foreground (moving objects) from the stationary background.\n",
    "* ##### This technique finds applications in various domains such as surveillance, object tracking, human-computer interaction, and video analysis.\n",
    "* ##### [Click to check the documentation](https://opencv24-python-tutorials.readthedocs.io/en/latest/py_tutorials/py_video/py_bg_subtraction/py_bg_subtraction.html)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated by user!\n"
     ]
    }
   ],
   "source": [
    "@error_handler\n",
    "def background_subtraction():\n",
    "    cap = cv.VideoCapture(0)\n",
    "    fg_bg = cv.createBackgroundSubtractorMOG2()\n",
    "    # fgbg=cv.bgsegm.createBackgroundSubtractorGMG()\n",
    "    # fgbg=cv.bgsegm.createBackgroundSubtractorMOG()\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        fg_mask = fg_bg.apply(frame)\n",
    "        cv.imshow('foreground mask', fg_mask)\n",
    "        cv.imshow('frame', frame)\n",
    "        k = cv.waitKey(30)\n",
    "        get_updated_value(k, 0)\n",
    "    cap.release()\n",
    "    cv.destroyAllWindows()\n",
    "\n",
    "background_subtraction()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Section 3, _Motion Detection_:\n",
    "* ##### It's the process of identifying and tracking movement or changes in a sequence of images or video frames.\n",
    "* ##### The goal is to detect regions or objects that are in motion, distinguishing them from the static background.\n",
    "* ##### [Click to check the course](https://www.coursera.org/learn/object-tracking-and-motion-computer-vision)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "@error_handler\n",
    "def motion_detection():\n",
    "    cap = cv.VideoCapture(0)\n",
    "    ret, prev = cap.read()\n",
    "    ret, current = cap.read()\n",
    "    while ret:\n",
    "        diff = cv.absdiff(prev, current)\n",
    "        gray = cv.cvtColor(diff, cv.COLOR_BGR2GRAY)\n",
    "        blur = cv.GaussianBlur(gray, (5, 5), 0)\n",
    "        ret, thresh = cv.threshold(blur, 20, 255, cv.THRESH_BINARY)\n",
    "        dilated = cv.dilate(thresh, np.ones((7, 7), np.uint8), iterations=9)\n",
    "        contours, _ = cv.findContours(dilated, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        cv.drawContours(prev, contours, -1, (0, 255, 0), 2)\n",
    "        cv.imshow(\"inter\", prev)\n",
    "        if cv.waitKey(40) == 27:\n",
    "            break\n",
    "        prev = current\n",
    "        ret, current = cap.read()\n",
    "    cv.destroyAllWindows()\n",
    "    cap.release()\n",
    "\n",
    "motion_detection()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## _The End_."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
